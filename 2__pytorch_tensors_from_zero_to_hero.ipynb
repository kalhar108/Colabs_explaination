{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u1rW8K2tEcI"
      },
      "source": [
        "# PyTorch Tensors: From Zero to Hero\n",
        "\n",
        "## A Comprehensive Guide to Tensor Operations for Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "1. **What are Tensors?** - The building blocks of deep learning\n",
        "2. **Creating Tensors** - Multiple ways to create and initialize\n",
        "3. **Tensor Attributes** - Shape, dtype, device\n",
        "4. **Indexing & Slicing** - Accessing tensor elements\n",
        "5. **Basic Operations** - Math, broadcasting, in-place ops\n",
        "6. **Reshaping Tensors** - view, reshape, squeeze, permute\n",
        "7. **Linear Algebra** - Matrix operations essential for DL\n",
        "8. **Einstein Summation** - Powerful notation for tensor ops\n",
        "9. **Common DL Operations** - Softmax, activations, normalization\n",
        "10. **Practical Patterns** - Real-world PyTorch snippets\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TbvA5URtEcK",
        "outputId": "5aa2246d-6fc3-425a-bcfa-b09747cad2af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.9.0+cpu\n",
            "CUDA Available: False\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch if needed (uncomment for Colab)\n",
        "# !pip install torch torchvision\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check PyTorch version and available devices\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWyUBGVltEcL"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 1: What is a Tensor?\n",
        "\n",
        "## The Universal Data Container\n",
        "\n",
        "A **tensor** is a generalization of vectors and matrices to potentially higher dimensions.\n",
        "\n",
        "```\n",
        "Scalar (0D tensor):  5                    <- just a number\n",
        "Vector (1D tensor):  [1, 2, 3]            <- a list of numbers\n",
        "Matrix (2D tensor):  [[1, 2], [3, 4]]     <- a table of numbers\n",
        "3D tensor:           [[[1,2],[3,4]], [[5,6],[7,8]]]  <- a \"cube\" of numbers\n",
        "```\n",
        "\n",
        "### Visual Representation\n",
        "\n",
        "```\n",
        "0D (Scalar)     1D (Vector)      2D (Matrix)         3D Tensor\n",
        "                                                     \n",
        "    [5]          [1 2 3 4]       [[1 2 3]            [[[1 2]\n",
        "                                  [4 5 6]              [3 4]]\n",
        "                                  [7 8 9]]            [[5 6]\n",
        "                                                       [7 8]]]\n",
        "    \n",
        "  shape: ()     shape: (4,)     shape: (3,3)        shape: (2,2,2)\n",
        "  0 axes        1 axis          2 axes              3 axes\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgh1cFXctEcM",
        "outputId": "404d3a59-a673-4cfc-8c5c-09b6b27d026b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 0D Tensor (Scalar) ===\n",
            "Value: 42\n",
            "Shape: torch.Size([])\n",
            "Dimensions: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's create tensors of different dimensions\n",
        "\n",
        "# 0D Tensor (Scalar)\n",
        "scalar = torch.tensor(42)\n",
        "print(\"=== 0D Tensor (Scalar) ===\")\n",
        "print(f\"Value: {scalar}\")\n",
        "print(f\"Shape: {scalar.shape}\")\n",
        "print(f\"Dimensions: {scalar.ndim}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePFZ6I8EtEcM",
        "outputId": "65445eef-cb73-421c-891a-728675f716e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1D Tensor (Vector) ===\n",
            "Value: tensor([1, 2, 3, 4, 5])\n",
            "Shape: torch.Size([5])\n",
            "Dimensions: 1\n",
            "Number of elements: 5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1D Tensor (Vector)\n",
        "vector = torch.tensor([1, 2, 3, 4, 5])\n",
        "print(\"=== 1D Tensor (Vector) ===\")\n",
        "print(f\"Value: {vector}\")\n",
        "print(f\"Shape: {vector.shape}\")\n",
        "print(f\"Dimensions: {vector.ndim}\")\n",
        "print(f\"Number of elements: {vector.numel()}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEbSlPLntEcM",
        "outputId": "f21e357c-bdf6-45e8-f601-3fec9daf563a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 2D Tensor (Matrix) ===\n",
            "Value:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "Shape: torch.Size([3, 3])\n",
            "Dimensions: 2\n",
            "Number of elements: 9\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2D Tensor (Matrix)\n",
        "matrix = torch.tensor([[1, 2, 3],\n",
        "                       [4, 5, 6],\n",
        "                       [7, 8, 9]])\n",
        "print(\"=== 2D Tensor (Matrix) ===\")\n",
        "print(f\"Value:\\n{matrix}\")\n",
        "print(f\"Shape: {matrix.shape}\")\n",
        "print(f\"Dimensions: {matrix.ndim}\")\n",
        "print(f\"Number of elements: {matrix.numel()}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGronPt4tEcM",
        "outputId": "a12d6c0a-7426-4ac9-d81b-269131d7433d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 3D Tensor ===\n",
            "Value:\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n",
            "Shape: torch.Size([2, 2, 2])\n",
            "Dimensions: 3\n",
            "Number of elements: 8\n"
          ]
        }
      ],
      "source": [
        "# 3D Tensor (think: batch of matrices, or RGB image)\n",
        "tensor_3d = torch.tensor([[[1, 2], [3, 4]],\n",
        "                          [[5, 6], [7, 8]]])\n",
        "print(\"=== 3D Tensor ===\")\n",
        "print(f\"Value:\\n{tensor_3d}\")\n",
        "print(f\"Shape: {tensor_3d.shape}\")\n",
        "print(f\"Dimensions: {tensor_3d.ndim}\")\n",
        "print(f\"Number of elements: {tensor_3d.numel()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN2i4dzrtEcN"
      },
      "source": [
        "### Why Tensors in Deep Learning?\n",
        "\n",
        "| Data Type | Tensor Shape | Example |\n",
        "|-----------|--------------|----------|\n",
        "| Single number | `()` | Loss value: `0.5` |\n",
        "| Feature vector | `(features,)` | Word embedding: `(300,)` |\n",
        "| Batch of vectors | `(batch, features)` | Batch of embeddings: `(32, 300)` |\n",
        "| Grayscale image | `(height, width)` | MNIST digit: `(28, 28)` |\n",
        "| Color image | `(channels, height, width)` | RGB image: `(3, 224, 224)` |\n",
        "| Batch of images | `(batch, channels, height, width)` | Image batch: `(32, 3, 224, 224)` |\n",
        "| Sequence | `(sequence_length, features)` | Time series: `(100, 10)` |\n",
        "| Batch of sequences | `(batch, sequence, features)` | Text batch: `(32, 50, 512)` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRnP8qUJtEcN"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 2: Creating Tensors\n",
        "\n",
        "PyTorch offers many ways to create tensors. Let's explore them all!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsGEFeu_tEcN"
      },
      "source": [
        "## 2.1 From Python Lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AM-QWprtEcN",
        "outputId": "6479f9a9-5fe1-49ee-9100-ff05333e630c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From list: tensor([1, 2, 3, 4])\n",
            "Dtype: torch.int64\n",
            "\n",
            "From nested list:\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "\n",
            "With floats: tensor([1., 2., 3.])\n",
            "Dtype: torch.float32\n"
          ]
        }
      ],
      "source": [
        "# From a Python list\n",
        "t1 = torch.tensor([1, 2, 3, 4])\n",
        "print(f\"From list: {t1}\")\n",
        "print(f\"Dtype: {t1.dtype}\")  # Default: int64 for integers\n",
        "\n",
        "# From nested lists (creates 2D tensor)\n",
        "t2 = torch.tensor([[1, 2], [3, 4]])\n",
        "print(f\"\\nFrom nested list:\\n{t2}\")\n",
        "\n",
        "# With floating point numbers\n",
        "t3 = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(f\"\\nWith floats: {t3}\")\n",
        "print(f\"Dtype: {t3.dtype}\")  # Default: float32 for floats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxshodL2tEcN"
      },
      "source": [
        "## 2.2 From NumPy Arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imsx4t4PtEcN",
        "outputId": "b21dd10f-8350-4d15-9a6f-1244ead27e8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy array:\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "\n",
            "PyTorch tensor:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "After modifying NumPy array:\n",
            "NumPy: 999\n",
            "Tensor: 999\n",
            "They share the same memory!\n"
          ]
        }
      ],
      "source": [
        "# From NumPy array\n",
        "np_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "tensor_from_np = torch.from_numpy(np_array)\n",
        "\n",
        "print(f\"NumPy array:\\n{np_array}\")\n",
        "print(f\"\\nPyTorch tensor:\\n{tensor_from_np}\")\n",
        "\n",
        "# IMPORTANT: They share memory!\n",
        "np_array[0, 0] = 999\n",
        "print(f\"\\nAfter modifying NumPy array:\")\n",
        "print(f\"NumPy: {np_array[0, 0]}\")\n",
        "print(f\"Tensor: {tensor_from_np[0, 0]}\")\n",
        "print(\"They share the same memory!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFWXBEmltEcO",
        "outputId": "1e4d63c9-1abe-40d7-fa11-643795a8a68b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy modified: [999   2   3]\n",
            "Tensor unchanged: tensor([1, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "# To avoid memory sharing, use .clone() or torch.tensor()\n",
        "np_array2 = np.array([1, 2, 3])\n",
        "tensor_copy = torch.tensor(np_array2)  # Creates a copy\n",
        "\n",
        "np_array2[0] = 999\n",
        "print(f\"NumPy modified: {np_array2}\")\n",
        "print(f\"Tensor unchanged: {tensor_copy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHuT2YHvtEcO"
      },
      "source": [
        "## 2.3 Special Initialization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH1h0-8ItEcO",
        "outputId": "6a34ceb9-4cd0-4000-df56-87eff83a1b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.zeros(3, 4):\n",
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Zeros - useful for bias initialization\n",
        "zeros = torch.zeros(3, 4)\n",
        "print(\"torch.zeros(3, 4):\")\n",
        "print(zeros)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMYqpHYytEcO",
        "outputId": "baae962d-9e07-4f0f-fe89-302584c8e19b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.ones(2, 3):\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ones\n",
        "ones = torch.ones(2, 3)\n",
        "print(\"torch.ones(2, 3):\")\n",
        "print(ones)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVDHUcZHtEcO",
        "outputId": "6c9eea91-bad4-4ee0-c0c7-6679a3700e29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.full((2, 3), 3.14):\n",
            "tensor([[3.1400, 3.1400, 3.1400],\n",
            "        [3.1400, 3.1400, 3.1400]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Full - fill with any value\n",
        "full = torch.full((2, 3), fill_value=3.14)\n",
        "print(\"torch.full((2, 3), 3.14):\")\n",
        "print(full)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c78lSH3GtEcO",
        "outputId": "15256971-ca7a-4142-fa6f-d78112def053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.eye(4):\n",
            "tensor([[1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Identity matrix - crucial for linear algebra\n",
        "eye = torch.eye(4)\n",
        "print(\"torch.eye(4):\")\n",
        "print(eye)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZXR40ZNtEcO",
        "outputId": "27a24aa6-872b-40d2-c335-352cf16b106d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.arange(0, 10, 2): tensor([0, 2, 4, 6, 8])\n",
            "torch.linspace(0, 1, 5): tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "# Arange - like Python's range()\n",
        "arange = torch.arange(0, 10, 2)  # start, end (exclusive), step\n",
        "print(f\"torch.arange(0, 10, 2): {arange}\")\n",
        "\n",
        "# Linspace - evenly spaced values\n",
        "linspace = torch.linspace(0, 1, 5)  # start, end, num_points\n",
        "print(f\"torch.linspace(0, 1, 5): {linspace}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUSdOOAgtEcO"
      },
      "source": [
        "## 2.4 Random Tensors (Essential for Weight Initialization!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toI1PYoftEcO",
        "outputId": "41bee343-9aff-4b1b-bfd8-8c36bfefe6ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.rand(3, 3) - Uniform [0, 1):\n",
            "tensor([[0.8823, 0.9150, 0.3829],\n",
            "        [0.9593, 0.3904, 0.6009],\n",
            "        [0.2566, 0.7936, 0.9408]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Uniform distribution [0, 1)\n",
        "uniform = torch.rand(3, 3)\n",
        "print(\"torch.rand(3, 3) - Uniform [0, 1):\")\n",
        "print(uniform)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_UkQbF6tEcO",
        "outputId": "8d9a3f57-adee-43c5-d3dd-69c381185956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.randn(3, 3) - Normal(0, 1):\n",
            "tensor([[ 1.5231,  0.6647, -1.0324],\n",
            "        [-0.2770, -0.1671, -0.1079],\n",
            "        [-1.4285, -0.2810,  0.7489]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Normal distribution (mean=0, std=1)\n",
        "normal = torch.randn(3, 3)\n",
        "print(\"torch.randn(3, 3) - Normal(0, 1):\")\n",
        "print(normal)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Krc_bA4ZtEcO",
        "outputId": "2d4525b2-3670-4ab6-b708-249d46535520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.randint(0, 10, (3, 3)):\n",
            "tensor([[9, 2, 0],\n",
            "        [5, 9, 3],\n",
            "        [4, 9, 6]])\n"
          ]
        }
      ],
      "source": [
        "# Random integers\n",
        "randint = torch.randint(low=0, high=10, size=(3, 3))\n",
        "print(\"torch.randint(0, 10, (3, 3)):\")\n",
        "print(randint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvDYjbZatEcO",
        "outputId": "d80b62a7-d2ce-49fd-c137-d6f22315f4ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x300 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Visualize distributions\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
        "\n",
        "# Uniform\n",
        "uniform_samples = torch.rand(10000)\n",
        "axes[0].hist(uniform_samples.numpy(), bins=50, edgecolor='black')\n",
        "axes[0].set_title('Uniform Distribution\\ntorch.rand()')\n",
        "axes[0].set_xlabel('Value')\n",
        "\n",
        "# Normal\n",
        "normal_samples = torch.randn(10000)\n",
        "axes[1].hist(normal_samples.numpy(), bins=50, edgecolor='black')\n",
        "axes[1].set_title('Normal Distribution\\ntorch.randn()')\n",
        "axes[1].set_xlabel('Value')\n",
        "\n",
        "# Random integers\n",
        "int_samples = torch.randint(0, 10, (10000,))\n",
        "axes[2].hist(int_samples.numpy(), bins=10, edgecolor='black')\n",
        "axes[2].set_title('Random Integers\\ntorch.randint()')\n",
        "axes[2].set_xlabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drQpaIHWtEcP"
      },
      "source": [
        "## 2.5 Creating Tensors Like Other Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcXsJERUtEcP",
        "outputId": "00e130a3-01f2-4c7c-b15b-17b76f26864b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference tensor (shape=torch.Size([2, 2]), dtype=torch.float32)\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "\n",
            "zeros_like:\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "\n",
            "ones_like:\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "\n",
            "rand_like:\n",
            "tensor([[0.2296, 0.6819],\n",
            "        [0.7305, 0.9644]])\n"
          ]
        }
      ],
      "source": [
        "# Create a reference tensor\n",
        "reference = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "print(f\"Reference tensor (shape={reference.shape}, dtype={reference.dtype})\")\n",
        "print(reference)\n",
        "print()\n",
        "\n",
        "# Create tensors with same shape and dtype\n",
        "zeros_like = torch.zeros_like(reference)\n",
        "ones_like = torch.ones_like(reference)\n",
        "rand_like = torch.rand_like(reference)\n",
        "\n",
        "print(f\"zeros_like:\\n{zeros_like}\\n\")\n",
        "print(f\"ones_like:\\n{ones_like}\\n\")\n",
        "print(f\"rand_like:\\n{rand_like}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part 3: Tensor Attributes - Shape, Dtype, Device\n",
        "\n",
        "Every tensor has three fundamental attributes you must understand."
      ],
      "metadata": {
        "id": "fS7MFJsItEcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Shape - The Dimensions of Your Data\n",
        "\n",
        "```\n",
        "Shape tells you how many elements along each axis.\n",
        "\n",
        "Example: shape (2, 3, 4) means:\n",
        "- 2 elements along axis 0\n",
        "- 3 elements along axis 1  \n",
        "- 4 elements along axis 2\n",
        "- Total: 2  3  4 = 24 elements\n",
        "```"
      ],
      "metadata": {
        "id": "iBEKiZBbtEcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape examples\n",
        "t = torch.randn(2, 3, 4)  # 3D tensor\n",
        "\n",
        "print(f\"Tensor shape: {t.shape}\")\n",
        "print(f\"Tensor shape (alternative): {t.size()}\")  # Same thing!\n",
        "print()\n",
        "\n",
        "# Access individual dimensions\n",
        "print(f\"Dimension 0 (batch): {t.shape[0]}\")\n",
        "print(f\"Dimension 1 (rows): {t.shape[1]}\")\n",
        "print(f\"Dimension 2 (cols): {t.shape[2]}\")\n",
        "print()\n",
        "\n",
        "# Useful properties\n",
        "print(f\"Number of dimensions: {t.ndim}\")\n",
        "print(f\"Total elements: {t.numel()}\")"
      ],
      "metadata": {
        "id": "SAWtYbVWtEcP",
        "outputId": "78ada1de-b0bc-428a-cf50-a44761824ed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor shape: torch.Size([2, 3, 4])\n",
            "Tensor shape (alternative): torch.Size([2, 3, 4])\n",
            "\n",
            "Dimension 0 (batch): 2\n",
            "Dimension 1 (rows): 3\n",
            "Dimension 2 (cols): 4\n",
            "\n",
            "Number of dimensions: 3\n",
            "Total elements: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Dtype - Data Type\n",
        "\n",
        "| Dtype | Description | Use Case |\n",
        "|-------|-------------|----------|\n",
        "| `torch.float32` (default) | 32-bit float | Neural network weights, most operations |\n",
        "| `torch.float64` | 64-bit float | High precision calculations |\n",
        "| `torch.float16` | 16-bit float | Mixed precision training (faster!) |\n",
        "| `torch.bfloat16` | Brain floating point | TPU/modern GPU training |\n",
        "| `torch.int64` (default for int) | 64-bit integer | Indices, labels |\n",
        "| `torch.int32` | 32-bit integer | Indices |\n",
        "| `torch.bool` | Boolean | Masks |"
      ],
      "metadata": {
        "id": "hyRWHGSxtEcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dtype examples\n",
        "int_tensor = torch.tensor([1, 2, 3])\n",
        "float_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "print(f\"Integer tensor dtype: {int_tensor.dtype}\")\n",
        "print(f\"Float tensor dtype: {float_tensor.dtype}\")\n",
        "print()\n",
        "\n",
        "# Specify dtype explicitly\n",
        "t_float64 = torch.tensor([1, 2, 3], dtype=torch.float64)\n",
        "t_float16 = torch.tensor([1, 2, 3], dtype=torch.float16)\n",
        "t_bool = torch.tensor([True, False, True], dtype=torch.bool)\n",
        "\n",
        "print(f\"float64: {t_float64}, dtype={t_float64.dtype}\")\n",
        "print(f\"float16: {t_float16}, dtype={t_float16.dtype}\")\n",
        "print(f\"bool: {t_bool}, dtype={t_bool.dtype}\")"
      ],
      "metadata": {
        "id": "8JIFG_OotEcP",
        "outputId": "5838a185-6d99-47f1-94a6-034c15b4a62b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integer tensor dtype: torch.int64\n",
            "Float tensor dtype: torch.float32\n",
            "\n",
            "float64: tensor([1., 2., 3.], dtype=torch.float64), dtype=torch.float64\n",
            "float16: tensor([1., 2., 3.], dtype=torch.float16), dtype=torch.float16\n",
            "bool: tensor([ True, False,  True]), dtype=torch.bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Type conversion (casting)\n",
        "original = torch.tensor([1, 2, 3])\n",
        "print(f\"Original: {original}, dtype={original.dtype}\")\n",
        "\n",
        "# Method 1: .to()\n",
        "as_float = original.to(torch.float32)\n",
        "print(f\"to(float32): {as_float}, dtype={as_float.dtype}\")\n",
        "\n",
        "# Method 2: .float(), .int(), .long(), .double()\n",
        "as_float2 = original.float()  # Same as .to(torch.float32)\n",
        "as_double = original.double()  # Same as .to(torch.float64)\n",
        "\n",
        "print(f\".float(): {as_float2}, dtype={as_float2.dtype}\")\n",
        "print(f\".double(): {as_double}, dtype={as_double.dtype}\")"
      ],
      "metadata": {
        "id": "ZWB_LLlPtEcP",
        "outputId": "ba199a95-26ae-440f-b96d-cac359811a38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: tensor([1, 2, 3]), dtype=torch.int64\n",
            "to(float32): tensor([1., 2., 3.]), dtype=torch.float32\n",
            ".float(): tensor([1., 2., 3.]), dtype=torch.float32\n",
            ".double(): tensor([1., 2., 3.], dtype=torch.float64), dtype=torch.float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Device - CPU vs GPU\n",
        "\n",
        "PyTorch can run on CPU or GPU (CUDA). GPU operations are **much faster** for large tensors!\n",
        "\n",
        "```\n",
        "CPU: Good for small tensors, debugging\n",
        "GPU: Essential for training neural networks\n",
        "```"
      ],
      "metadata": {
        "id": "piJyb2XUtEcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device\n",
        "cpu_tensor = torch.tensor([1, 2, 3])\n",
        "print(f\"Default device: {cpu_tensor.device}\")\n",
        "\n",
        "# Best practice: device-agnostic code\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create tensor on specific device\n",
        "tensor_on_device = torch.randn(3, 3, device=device)\n",
        "print(f\"Tensor device: {tensor_on_device.device}\")"
      ],
      "metadata": {
        "id": "bQV9YtLKtEcP",
        "outputId": "2ebfe6cd-e9b1-4f3c-b956-11c50cde241e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default device: cpu\n",
            "Using device: cpu\n",
            "Tensor device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving tensors between devices\n",
        "cpu_tensor = torch.randn(3, 3)\n",
        "\n",
        "# Move to GPU (if available)\n",
        "if torch.cuda.is_available():\n",
        "    gpu_tensor = cpu_tensor.to('cuda')  # or .cuda()\n",
        "    print(f\"GPU tensor device: {gpu_tensor.device}\")\n",
        "\n",
        "    # Move back to CPU\n",
        "    back_to_cpu = gpu_tensor.to('cpu')  # or .cpu()\n",
        "    print(f\"Back to CPU: {back_to_cpu.device}\")\n",
        "else:\n",
        "    print(\"CUDA not available - running on CPU only\")\n",
        "    print(\"This is fine for learning! GPU just makes things faster.\")"
      ],
      "metadata": {
        "id": "oBZBmmmVtEcP",
        "outputId": "82e0c8f1-129f-4c4c-e79c-9788ac7dbae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA not available - running on CPU only\n",
            "This is fine for learning! GPU just makes things faster.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part 4: Indexing and Slicing\n",
        "\n",
        "Access specific elements, rows, columns, or sub-tensors.\n",
        "\n",
        "```\n",
        "Indexing uses [ ] brackets, just like NumPy and Python lists!\n",
        "\n",
        "Key concepts:\n",
        "- Index starts at 0\n",
        "- Negative indices count from the end\n",
        "- Slicing: start:stop:step (stop is exclusive)\n",
        "- : means \"all elements\"\n",
        "```"
      ],
      "metadata": {
        "id": "lQ_Ll4_itEcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Basic Indexing - 1D Tensors"
      ],
      "metadata": {
        "id": "chDqQ63QtEcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 1D tensor\n",
        "v = torch.tensor([10, 20, 30, 40, 50])\n",
        "print(f\"Vector: {v}\")\n",
        "print()\n",
        "\n",
        "# Single element access\n",
        "print(f\"v[0] = {v[0]}\")      # First element\n",
        "print(f\"v[2] = {v[2]}\")      # Third element\n",
        "print(f\"v[-1] = {v[-1]}\")    # Last element\n",
        "print(f\"v[-2] = {v[-2]}\")    # Second to last\n",
        "print()\n",
        "\n",
        "# Slicing\n",
        "print(f\"v[1:4] = {v[1:4]}\")      # Elements 1, 2, 3 (4 is exclusive)\n",
        "print(f\"v[:3] = {v[:3]}\")        # First 3 elements\n",
        "print(f\"v[2:] = {v[2:]}\")        # From index 2 to end\n",
        "print(f\"v[::2] = {v[::2]}\")      # Every other element (step=2)\n",
        "print(f\"v[::-1] = {v[::-1]}\")    # Reverse!"
      ],
      "metadata": {
        "id": "muH-wLO8tEcP",
        "outputId": "fd31fffa-b798-4fb0-de23-00c2b32ebffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector: tensor([10, 20, 30, 40, 50])\n",
            "\n",
            "v[0] = 10\n",
            "v[2] = 30\n",
            "v[-1] = 50\n",
            "v[-2] = 40\n",
            "\n",
            "v[1:4] = tensor([20, 30, 40])\n",
            "v[:3] = tensor([10, 20, 30])\n",
            "v[2:] = tensor([30, 40, 50])\n",
            "v[::2] = tensor([10, 30, 50])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "step must be greater than zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4123210852.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"v[2:] = {v[2:]}\"\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# From index 2 to end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"v[::2] = {v[::2]}\"\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Every other element (step=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"v[::-1] = {v[::-1]}\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Reverse!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: step must be greater than zero"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 2D Indexing - Matrices\n",
        "\n",
        "```\n",
        "Matrix indexing: tensor[row, column]\n",
        "\n",
        "     Col 0  Col 1  Col 2\n",
        "Row 0  [1]    [2]    [3]\n",
        "Row 1  [4]    [5]    [6]\n",
        "Row 2  [7]    [8]    [9]\n",
        "\n",
        "m[1, 2] = 6  (row 1, column 2)\n",
        "```"
      ],
      "metadata": {
        "id": "YkZAgU1dtEcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 2D tensor (matrix)\n",
        "m = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6],\n",
        "                  [7, 8, 9]])\n",
        "print(f\"Matrix:\\n{m}\")\n",
        "print()\n",
        "\n",
        "# Single element\n",
        "print(f\"m[0, 0] = {m[0, 0]}\")  # Top-left\n",
        "print(f\"m[1, 2] = {m[1, 2]}\")  # Row 1, Col 2\n",
        "print(f\"m[-1, -1] = {m[-1, -1]}\")  # Bottom-right\n",
        "print()"
      ],
      "metadata": {
        "id": "S1jo9vZVtEcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Row and column selection\n",
        "print(\"=== Row Selection ===\")\n",
        "print(f\"m[0] = {m[0]}\")        # First row\n",
        "print(f\"m[0, :] = {m[0, :]}\")  # Same thing (explicit)\n",
        "print(f\"m[-1] = {m[-1]}\")      # Last row\n",
        "print()\n",
        "\n",
        "print(\"=== Column Selection ===\")\n",
        "print(f\"m[:, 0] = {m[:, 0]}\")  # First column\n",
        "print(f\"m[:, -1] = {m[:, -1]}\")  # Last column\n",
        "print()\n",
        "\n",
        "print(\"=== Submatrix Selection ===\")\n",
        "print(f\"m[0:2, 1:3] =\\\\n{m[0:2, 1:3]}\")  # Top-right 2x2"
      ],
      "metadata": {
        "id": "a3CZjA9_tEcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Advanced Indexing\n",
        "\n",
        "### Fancy Indexing (with lists/tensors)"
      ],
      "metadata": {
        "id": "0c1qrN10tEcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fancy indexing - select specific rows/cols by index list\n",
        "m = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6],\n",
        "                  [7, 8, 9]])\n",
        "\n",
        "# Select rows 0 and 2\n",
        "rows = torch.tensor([0, 2])\n",
        "print(f\"Select rows 0 and 2:\\\\n{m[rows]}\")\n",
        "print()\n",
        "\n",
        "# Select specific elements: (0,1), (1,2), (2,0)\n",
        "row_indices = torch.tensor([0, 1, 2])\n",
        "col_indices = torch.tensor([1, 2, 0])\n",
        "print(f\"Elements at (0,1), (1,2), (2,0): {m[row_indices, col_indices]}\")"
      ],
      "metadata": {
        "id": "34XLjE97tEcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boolean Indexing (Masking)\n",
        "\n",
        "Super useful for filtering data!"
      ],
      "metadata": {
        "id": "z197_qlStEcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boolean indexing\n",
        "t = torch.tensor([1, -2, 3, -4, 5, -6])\n",
        "\n",
        "# Create a boolean mask\n",
        "positive_mask = t > 0\n",
        "print(f\"Tensor: {t}\")\n",
        "print(f\"Positive mask: {positive_mask}\")\n",
        "print(f\"Positive elements: {t[positive_mask]}\")\n",
        "print()\n",
        "\n",
        "# One-liner filtering\n",
        "print(f\"Elements > 2: {t[t > 2]}\")\n",
        "print(f\"Elements between -3 and 3: {t[(t > -3) & (t < 3)]}\")"
      ],
      "metadata": {
        "id": "kRe_53yetEcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part 5: Basic Tensor Operations\n",
        "\n",
        "Now let's do math with tensors!"
      ],
      "metadata": {
        "id": "wQkEQgS9tEcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Element-wise Arithmetic\n",
        "\n",
        "Operations applied to each element independently."
      ],
      "metadata": {
        "id": "x0EFagDbtEcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "print(f\"a = {a}\")\n",
        "print(f\"b = {b}\")\n",
        "print()\n",
        "\n",
        "# Basic arithmetic (element-wise)\n",
        "print(f\"a + b = {a + b}\")       # Addition\n",
        "print(f\"a - b = {a - b}\")       # Subtraction\n",
        "print(f\"a * b = {a * b}\")       # Multiplication (element-wise!)\n",
        "print(f\"a / b = {a / b}\")       # Division\n",
        "print(f\"a ** 2 = {a ** 2}\")     # Power\n",
        "print(f\"a // 2 = {a // 2}\")     # Floor division\n",
        "print(f\"a % 2 = {a % 2}\")       # Modulo"
      ],
      "metadata": {
        "id": "iq7i4FYXtEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function equivalents (same results)\n",
        "print(\"=== Function equivalents ===\")\n",
        "print(f\"torch.add(a, b) = {torch.add(a, b)}\")\n",
        "print(f\"torch.sub(a, b) = {torch.sub(a, b)}\")\n",
        "print(f\"torch.mul(a, b) = {torch.mul(a, b)}\")\n",
        "print(f\"torch.div(a, b) = {torch.div(a, b)}\")"
      ],
      "metadata": {
        "id": "6W_PywD7tEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Common Mathematical Functions"
      ],
      "metadata": {
        "id": "MzqNReOrtEcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([1.0, 4.0, 9.0, 16.0])\n",
        "\n",
        "print(f\"Original: {t}\")\n",
        "print()\n",
        "\n",
        "# Common math functions\n",
        "print(f\"sqrt: {torch.sqrt(t)}\")\n",
        "print(f\"exp: {torch.exp(torch.tensor([0.0, 1.0, 2.0]))}\")\n",
        "print(f\"log: {torch.log(t)}\")\n",
        "print(f\"log10: {torch.log10(t)}\")\n",
        "print()\n",
        "\n",
        "# Trigonometric\n",
        "angles = torch.tensor([0.0, 3.14159/4, 3.14159/2])\n",
        "print(f\"sin: {torch.sin(angles)}\")\n",
        "print(f\"cos: {torch.cos(angles)}\")\n",
        "print()\n",
        "\n",
        "# Rounding\n",
        "floats = torch.tensor([1.2, 2.5, 3.7, -1.3])\n",
        "print(f\"Original: {floats}\")\n",
        "print(f\"floor: {torch.floor(floats)}\")\n",
        "print(f\"ceil: {torch.ceil(floats)}\")\n",
        "print(f\"round: {torch.round(floats)}\")"
      ],
      "metadata": {
        "id": "DzqKRpuptEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Aggregation Functions (Reduce Operations)\n",
        "\n",
        "These functions reduce a tensor to fewer elements."
      ],
      "metadata": {
        "id": "Ki1b6GVvtEcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([[1.0, 2.0, 3.0],\n",
        "                  [4.0, 5.0, 6.0]])\n",
        "print(f\"Tensor (shape={t.shape}):\\\\n{t}\")\n",
        "print()\n",
        "\n",
        "# Global aggregations (all elements)\n",
        "print(\"=== Global Aggregations ===\")\n",
        "print(f\"sum: {t.sum()}\")\n",
        "print(f\"mean: {t.mean()}\")\n",
        "print(f\"std: {t.std()}\")\n",
        "print(f\"var: {t.var()}\")\n",
        "print(f\"min: {t.min()}\")\n",
        "print(f\"max: {t.max()}\")\n",
        "print(f\"prod: {t.prod()}\")  # Product of all elements"
      ],
      "metadata": {
        "id": "ylwBiuuQtEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregation along specific dimensions\n",
        "print(\"=== Dimension-wise Aggregations ===\")\n",
        "print(f\"Original:\\\\n{t}\")\n",
        "print()\n",
        "\n",
        "# dim=0: reduce rows (aggregate down columns)\n",
        "print(f\"sum(dim=0) - sum down columns: {t.sum(dim=0)}\")\n",
        "\n",
        "# dim=1: reduce columns (aggregate across rows)\n",
        "print(f\"sum(dim=1) - sum across rows: {t.sum(dim=1)}\")\n",
        "print()\n",
        "\n",
        "# Keep dimensions (useful for broadcasting)\n",
        "print(f\"sum(dim=1, keepdim=True):\\\\n{t.sum(dim=1, keepdim=True)}\")"
      ],
      "metadata": {
        "id": "aP3iQNA9tEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize dimension reduction\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
        "\n",
        "# Original matrix\n",
        "ax = axes[0]\n",
        "ax.imshow([[1,2,3],[4,5,6]], cmap='Blues')\n",
        "ax.set_title('Original (2x3)')\n",
        "for i in range(2):\n",
        "    for j in range(3):\n",
        "        ax.text(j, i, f'{t[i,j].item():.0f}', ha='center', va='center', fontsize=14)\n",
        "ax.set_xticks([0,1,2])\n",
        "ax.set_yticks([0,1])\n",
        "\n",
        "# Sum dim=0\n",
        "ax = axes[1]\n",
        "ax.imshow([[5,7,9]], cmap='Greens')\n",
        "ax.set_title('sum(dim=0)  (3,)')\n",
        "for j, val in enumerate([5,7,9]):\n",
        "    ax.text(j, 0, f'{val}', ha='center', va='center', fontsize=14)\n",
        "ax.set_xticks([0,1,2])\n",
        "ax.set_yticks([0])\n",
        "ax.set_ylabel(' collapse')\n",
        "\n",
        "# Sum dim=1\n",
        "ax = axes[2]\n",
        "ax.imshow([[6],[15]], cmap='Oranges')\n",
        "ax.set_title('sum(dim=1)  (2,)')\n",
        "for i, val in enumerate([6,15]):\n",
        "    ax.text(0, i, f'{val}', ha='center', va='center', fontsize=14)\n",
        "ax.set_xticks([0])\n",
        "ax.set_yticks([0,1])\n",
        "ax.set_xlabel(' collapse')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O5NOfe5RtEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding indices of min/max (crucial for classification!)\n",
        "scores = torch.tensor([[0.1, 0.7, 0.2],\n",
        "                       [0.8, 0.1, 0.1],\n",
        "                       [0.3, 0.3, 0.4]])\n",
        "print(f\"Class scores (3 samples, 3 classes):\\\\n{scores}\")\n",
        "print()\n",
        "\n",
        "# argmax - index of maximum value\n",
        "predictions = torch.argmax(scores, dim=1)\n",
        "print(f\"Predicted classes (argmax along dim=1): {predictions}\")\n",
        "\n",
        "# argmin\n",
        "print(f\"Least likely classes (argmin along dim=1): {torch.argmin(scores, dim=1)}\")"
      ],
      "metadata": {
        "id": "GkJwa5MFtEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Broadcasting - Automatic Shape Expansion\n",
        "\n",
        "Broadcasting allows operations between tensors of different shapes. PyTorch automatically \"expands\" smaller tensors to match larger ones.\n",
        "\n",
        "### Broadcasting Rules:\n",
        "1. Align shapes from the right\n",
        "2. Dimensions are compatible if they're equal OR one of them is 1\n",
        "3. Missing dimensions are treated as 1\n",
        "\n",
        "```\n",
        "Example: Adding (3,4) and (4,)\n",
        "\n",
        "     (3, 4)      Shape of tensor A\n",
        "        (4,)     Shape of tensor B  (aligned from right)\n",
        "     ------\n",
        "     (3, 4)     Result: B is broadcast to (3, 4)\n",
        "```"
      ],
      "metadata": {
        "id": "n0zQ0k_2tEcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Broadcasting examples\n",
        "\n",
        "# Scalar + Tensor\n",
        "t = torch.tensor([1, 2, 3])\n",
        "print(f\"[1,2,3] + 10 = {t + 10}\")\n",
        "print()\n",
        "\n",
        "# Vector + Matrix (row-wise)\n",
        "matrix = torch.tensor([[1, 2, 3],\n",
        "                       [4, 5, 6]])\n",
        "row = torch.tensor([10, 20, 30])\n",
        "\n",
        "print(f\"Matrix (2,3):\\\\n{matrix}\")\n",
        "print(f\"Row (3,): {row}\")\n",
        "print(f\"Matrix + Row:\\\\n{matrix + row}\")\n",
        "print(\"Each row gets the vector added to it!\")"
      ],
      "metadata": {
        "id": "NgxYTr4PtEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column-wise broadcasting (need to add dimension!)\n",
        "matrix = torch.tensor([[1, 2, 3],\n",
        "                       [4, 5, 6]])\n",
        "col = torch.tensor([[100], [200]])  # Shape (2, 1)\n",
        "\n",
        "print(f\"Matrix (2,3):\\\\n{matrix}\")\n",
        "print(f\"Column (2,1):\\\\n{col}\")\n",
        "print(f\"Matrix + Column:\\\\n{matrix + col}\")\n",
        "print(\"Each column gets the vector added to it!\")"
      ],
      "metadata": {
        "id": "uXNq3BeutEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 In-Place Operations\n",
        "\n",
        "Operations ending with `_` modify tensors in-place (save memory but be careful with autograd!)."
      ],
      "metadata": {
        "id": "cv_FRcQgtEcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In-place operations (underscore suffix)\n",
        "t = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(f\"Original: {t}\")\n",
        "\n",
        "# Normal operation (creates new tensor)\n",
        "t2 = t + 10\n",
        "print(f\"t + 10: {t2}\")\n",
        "print(f\"Original unchanged: {t}\")\n",
        "print()\n",
        "\n",
        "# In-place operation (modifies original!)\n",
        "t.add_(10)  # Same as t += 10\n",
        "print(f\"After t.add_(10): {t}\")\n",
        "\n",
        "# Common in-place operations\n",
        "t.mul_(2)   # t *= 2\n",
        "t.sub_(5)   # t -= 5\n",
        "t.div_(2)   # t /= 2\n",
        "print(f\"After more in-place ops: {t}\")"
      ],
      "metadata": {
        "id": "Tx2tZpY7tEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part 6: Reshaping Tensors\n",
        "\n",
        "Reshaping is essential for connecting different layers in neural networks!\n",
        "\n",
        "```\n",
        "Examples of when you need reshaping:\n",
        "- Flatten images before a fully-connected layer\n",
        "- Add batch dimension for single samples\n",
        "- Rearrange dimensions for convolutions\n",
        "- Combine or split batch/sequence dimensions\n",
        "```"
      ],
      "metadata": {
        "id": "HVdRbLvKtEcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 view() and reshape()\n",
        "\n",
        "Change the shape while keeping all elements."
      ],
      "metadata": {
        "id": "AHXRHXRYtEcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# view() - changes shape (must be contiguous)\n",
        "t = torch.arange(12)  # [0,1,2,...,11]\n",
        "print(f\"Original: {t}, shape={t.shape}\")\n",
        "\n",
        "# Reshape to 2D\n",
        "t_2d = t.view(3, 4)\n",
        "print(f\"view(3, 4):\\\\n{t_2d}\")\n",
        "\n",
        "t_2d_alt = t.view(4, 3)\n",
        "print(f\"view(4, 3):\\\\n{t_2d_alt}\")\n",
        "\n",
        "# Reshape to 3D\n",
        "t_3d = t.view(2, 2, 3)\n",
        "print(f\"view(2, 2, 3):\\\\n{t_3d}\")"
      ],
      "metadata": {
        "id": "RcXxAy1BtEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using -1 to infer dimension automatically\n",
        "t = torch.arange(12)\n",
        "\n",
        "print(f\"view(3, -1): {t.view(3, -1).shape}\")  # PyTorch figures out it's 4\n",
        "print(f\"view(-1, 6): {t.view(-1, 6).shape}\")  # PyTorch figures out it's 2\n",
        "print(f\"view(2, -1, 3): {t.view(2, -1, 3).shape}\")  # Middle dim is 2\n",
        "print()\n",
        "\n",
        "# reshape() vs view(): reshape works even if tensor is not contiguous\n",
        "# Generally prefer reshape() as it's more flexible\n",
        "t_reshaped = t.reshape(3, 4)\n",
        "print(f\"reshape(3, 4):\\\\n{t_reshaped}\")"
      ],
      "metadata": {
        "id": "l99AAlVctEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 flatten() - Collapse Dimensions\n",
        "\n",
        "Essential for connecting convolutional layers to fully-connected layers!"
      ],
      "metadata": {
        "id": "cewdn6cHtEcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulating a batch of images\n",
        "# Shape: (batch=2, channels=3, height=4, width=4)\n",
        "images = torch.randn(2, 3, 4, 4)\n",
        "print(f\"Image batch shape: {images.shape}\")\n",
        "\n",
        "# Flatten completely\n",
        "flat = images.flatten()\n",
        "print(f\"flatten(): {flat.shape}\")\n",
        "\n",
        "# Flatten starting from dimension 1 (keep batch)\n",
        "# This is what you do before a Linear layer!\n",
        "flat_keep_batch = images.flatten(start_dim=1)\n",
        "print(f\"flatten(start_dim=1): {flat_keep_batch.shape}\")\n",
        "print(\"Now each image is a single vector of 48 features (3*4*4)\")\n",
        "\n",
        "# Alternative using view\n",
        "flat_view = images.view(images.size(0), -1)\n",
        "print(f\"view(batch, -1): {flat_view.shape}\")"
      ],
      "metadata": {
        "id": "QwPhTgw3tEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 squeeze() and unsqueeze() - Add/Remove Dimensions\n",
        "\n",
        "```\n",
        "unsqueeze: Add a dimension of size 1\n",
        "squeeze: Remove dimensions of size 1\n",
        "```"
      ],
      "metadata": {
        "id": "E3NA6mg-tEcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unsqueeze - add a dimension\n",
        "t = torch.tensor([1, 2, 3])\n",
        "print(f\"Original: shape={t.shape}\")\n",
        "\n",
        "# Add dimension at position 0 (batch dimension)\n",
        "t_batch = t.unsqueeze(0)\n",
        "print(f\"unsqueeze(0): shape={t_batch.shape}\")\n",
        "print(t_batch)\n",
        "\n",
        "# Add dimension at position 1\n",
        "t_col = t.unsqueeze(1)\n",
        "print(f\"unsqueeze(1): shape={t_col.shape}\")\n",
        "print(t_col)\n",
        "\n",
        "# Alternative: use None/newaxis in indexing\n",
        "t_batch_alt = t[None, :]  # Same as unsqueeze(0)\n",
        "t_col_alt = t[:, None]    # Same as unsqueeze(1)\n",
        "print(f\"t[None, :]: shape={t_batch_alt.shape}\")\n",
        "print(f\"t[:, None]: shape={t_col_alt.shape}\")"
      ],
      "metadata": {
        "id": "rDCclDzNtEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# squeeze - remove dimensions of size 1\n",
        "t = torch.randn(1, 3, 1, 4)\n",
        "print(f\"Original: shape={t.shape}\")\n",
        "\n",
        "# Squeeze all dimensions of size 1\n",
        "t_squeezed = t.squeeze()\n",
        "print(f\"squeeze(): shape={t_squeezed.shape}\")\n",
        "\n",
        "# Squeeze specific dimension\n",
        "t_squeeze_0 = t.squeeze(0)\n",
        "print(f\"squeeze(0): shape={t_squeeze_0.shape}\")\n",
        "\n",
        "t_squeeze_2 = t.squeeze(2)\n",
        "print(f\"squeeze(2): shape={t_squeeze_2.shape}\")"
      ],
      "metadata": {
        "id": "zq-vzQHNtEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4 transpose() and permute() - Rearrange Dimensions\n",
        "\n",
        "```\n",
        "transpose: Swap two dimensions\n",
        "permute: Rearrange all dimensions in any order\n",
        "```"
      ],
      "metadata": {
        "id": "azglU-vYtEcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transpose - swap two dimensions\n",
        "m = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "print(f\"Original (2, 3):\\\\n{m}\")\n",
        "\n",
        "# 2D transpose (like matrix transpose)\n",
        "m_T = m.T  # or m.t() or m.transpose(0, 1)\n",
        "print(f\"Transpose (3, 2):\\\\n{m_T}\")\n",
        "\n",
        "# For higher dimensions, specify which dims to swap\n",
        "t = torch.randn(2, 3, 4)\n",
        "print(f\"\\\\n3D tensor: {t.shape}\")\n",
        "print(f\"transpose(0, 2): {t.transpose(0, 2).shape}\")  # Swap first and last"
      ],
      "metadata": {
        "id": "Tz4Zr272tEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# permute - reorder all dimensions\n",
        "# Common use: convert between channel orders\n",
        "\n",
        "# Image in HWC format (Height, Width, Channels) - NumPy/TensorFlow style\n",
        "img_hwc = torch.randn(224, 224, 3)\n",
        "print(f\"HWC format: {img_hwc.shape}\")\n",
        "\n",
        "# Convert to CHW format (Channels, Height, Width) - PyTorch style\n",
        "img_chw = img_hwc.permute(2, 0, 1)\n",
        "print(f\"CHW format: {img_chw.shape}\")\n",
        "\n",
        "# Batch of images: NHWC -> NCHW\n",
        "batch_nhwc = torch.randn(32, 224, 224, 3)\n",
        "batch_nchw = batch_nhwc.permute(0, 3, 1, 2)\n",
        "print(f\"\\\\nNHWC: {batch_nhwc.shape} -> NCHW: {batch_nchw.shape}\")"
      ],
      "metadata": {
        "id": "gw2PxobqtEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5 cat() and stack() - Combining Tensors\n",
        "\n",
        "```\n",
        "cat: Concatenate along existing dimension\n",
        "stack: Stack along NEW dimension\n",
        "```"
      ],
      "metadata": {
        "id": "T3BCSXebtEcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cat - concatenate along existing dimension\n",
        "a = torch.tensor([[1, 2], [3, 4]])\n",
        "b = torch.tensor([[5, 6], [7, 8]])\n",
        "print(f\"a:\\\\n{a}\")\n",
        "print(f\"b:\\\\n{b}\")\n",
        "print()\n",
        "\n",
        "# Concatenate along rows (dim=0)\n",
        "cat_rows = torch.cat([a, b], dim=0)\n",
        "print(f\"cat([a, b], dim=0):\\\\n{cat_rows}\")\n",
        "print(f\"Shape: {cat_rows.shape}\")\n",
        "print()\n",
        "\n",
        "# Concatenate along columns (dim=1)\n",
        "cat_cols = torch.cat([a, b], dim=1)\n",
        "print(f\"cat([a, b], dim=1):\\\\n{cat_cols}\")\n",
        "print(f\"Shape: {cat_cols.shape}\")"
      ],
      "metadata": {
        "id": "rqTI4zUdtEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stack - creates NEW dimension\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([4, 5, 6])\n",
        "c = torch.tensor([7, 8, 9])\n",
        "\n",
        "# Stack along dim 0 (creates batch dimension)\n",
        "stacked = torch.stack([a, b, c], dim=0)\n",
        "print(f\"stack([a,b,c], dim=0):\\\\n{stacked}\")\n",
        "print(f\"Shape: {stacked.shape}\")\n",
        "print()\n",
        "\n",
        "# Stack along dim 1\n",
        "stacked_1 = torch.stack([a, b, c], dim=1)\n",
        "print(f\"stack([a,b,c], dim=1):\\\\n{stacked_1}\")\n",
        "print(f\"Shape: {stacked_1.shape}\")\n",
        "print()\n",
        "\n",
        "# Common use: create batch from list of samples\n",
        "samples = [torch.randn(3, 32, 32) for _ in range(16)]  # 16 images\n",
        "batch = torch.stack(samples, dim=0)\n",
        "print(f\"Batch from {len(samples)} images: {batch.shape}\")"
      ],
      "metadata": {
        "id": "g5bY4YsjtEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part 7: Linear Algebra Operations\n",
        "\n",
        "Linear algebra is the mathematical foundation of deep learning!\n",
        "\n",
        "```\n",
        "Key operations:\n",
        "- Matrix multiplication (most important!)\n",
        "- Dot product\n",
        "- Norms\n",
        "- Matrix decompositions\n",
        "```"
      ],
      "metadata": {
        "id": "fCMP3UmrtEcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Matrix Multiplication - The Heart of Deep Learning\n",
        "\n",
        "Every neural network layer performs: `output = input @ weights + bias`\n",
        "\n",
        "```\n",
        "Matrix multiplication rule: (M, K) @ (K, N) = (M, N)\n",
        "The inner dimensions must match!\n",
        "\n",
        "Visual:\n",
        "    [a b c]       [1 2]       [a*1+b*3+c*5  a*2+b*4+c*6]\n",
        "    [d e f]   @   [3 4]   =   [d*1+e*3+f*5  d*2+e*4+f*6]\n",
        "                  [5 6]\n",
        "    (2, 3)    @   (3, 2)   =  (2, 2)\n",
        "```"
      ],
      "metadata": {
        "id": "Hy2_2_GQtEcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix multiplication\n",
        "A = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]], dtype=torch.float32)  # (2, 3)\n",
        "\n",
        "B = torch.tensor([[1, 2],\n",
        "                  [3, 4],\n",
        "                  [5, 6]], dtype=torch.float32)  # (3, 2)\n",
        "\n",
        "print(f\"A (2, 3):\\\\n{A}\")\n",
        "print(f\"B (3, 2):\\\\n{B}\")\n",
        "print()\n",
        "\n",
        "# Three ways to do matrix multiplication (all equivalent!)\n",
        "C1 = A @ B            # Operator (Python 3.5+)\n",
        "C2 = torch.mm(A, B)   # Matrix multiply function\n",
        "C3 = torch.matmul(A, B)  # General matmul (handles batches too)\n",
        "C4 = A.mm(B)          # Method form\n",
        "\n",
        "print(f\"A @ B =\\\\n{C1}\")\n",
        "print(f\"Result shape: {C1.shape}\")"
      ],
      "metadata": {
        "id": "S8vnXCHAtEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batched matrix multiplication\n",
        "# torch.matmul handles batches automatically!\n",
        "\n",
        "batch_A = torch.randn(32, 64, 128)  # 32 matrices of (64, 128)\n",
        "batch_B = torch.randn(32, 128, 256) # 32 matrices of (128, 256)\n",
        "\n",
        "result = torch.matmul(batch_A, batch_B)  # or batch_A @ batch_B\n",
        "print(f\"Batch matmul: {batch_A.shape} @ {batch_B.shape} = {result.shape}\")\n",
        "\n",
        "# Broadcasting in matmul\n",
        "# Single weight matrix applied to batch of inputs\n",
        "batch_input = torch.randn(32, 100)  # 32 samples, 100 features\n",
        "weights = torch.randn(100, 50)      # Transform to 50 features\n",
        "\n",
        "output = batch_input @ weights\n",
        "print(f\"Batched linear: {batch_input.shape} @ {weights.shape} = {output.shape}\")"
      ],
      "metadata": {
        "id": "L-eAlrBFtEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Layer = Matrix Multiplication!\n",
        "# Linear layer: y = xW + b\n",
        "\n",
        "batch_size = 32\n",
        "in_features = 784   # e.g., flattened 28x28 image\n",
        "out_features = 128\n",
        "\n",
        "# Input: batch of feature vectors\n",
        "X = torch.randn(batch_size, in_features)\n",
        "\n",
        "# Learnable parameters\n",
        "W = torch.randn(in_features, out_features)  # Weight matrix\n",
        "b = torch.randn(out_features)               # Bias vector\n",
        "\n",
        "# Forward pass: y = xW + b\n",
        "Y = X @ W + b  # Broadcasting adds b to each row\n",
        "\n",
        "print(f\"Input X: {X.shape}\")\n",
        "print(f\"Weights W: {W.shape}\")\n",
        "print(f\"Bias b: {b.shape}\")\n",
        "print(f\"Output Y: {Y.shape}\")\n",
        "print(\"\\\\nThis is exactly what nn.Linear does!\")"
      ],
      "metadata": {
        "id": "aFeYI9XLtEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 Dot Product and Inner Product"
      ],
      "metadata": {
        "id": "U8CUZ1dRtEcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dot product (for 1D vectors)\n",
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "# Dot product: sum of element-wise products\n",
        "dot = torch.dot(a, b)  # 1*4 + 2*5 + 3*6 = 32\n",
        "print(f\"a = {a}\")\n",
        "print(f\"b = {b}\")\n",
        "print(f\"dot(a, b) = {dot}\")\n",
        "print()\n",
        "\n",
        "# Equivalent ways:\n",
        "print(f\"(a * b).sum() = {(a * b).sum()}\")\n",
        "print(f\"a @ b = {a @ b}\")  # @ also works for 1D vectors"
      ],
      "metadata": {
        "id": "VgQBX_pEtEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 Vector and Matrix Norms\n",
        "\n",
        "Norms measure the \"size\" or \"magnitude\" of vectors/matrices.\n",
        "\n",
        "```\n",
        "L1 norm: ||x|| = |x|         (Manhattan distance)\n",
        "L2 norm: ||x|| = (x)      (Euclidean distance)\n",
        "Frobenius: ||A||_F = (a)  (Matrix \"size\")\n",
        "```"
      ],
      "metadata": {
        "id": "H2ToOEkEtEcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector norms\n",
        "v = torch.tensor([3.0, 4.0])\n",
        "\n",
        "# L2 norm (Euclidean) - default\n",
        "l2 = torch.norm(v)  # sqrt(3 + 4) = 5\n",
        "print(f\"v = {v}\")\n",
        "print(f\"L2 norm (default): {l2}\")\n",
        "\n",
        "# L1 norm (Manhattan)\n",
        "l1 = torch.norm(v, p=1)  # |3| + |4| = 7\n",
        "print(f\"L1 norm: {l1}\")\n",
        "\n",
        "# Infinity norm (max absolute value)\n",
        "linf = torch.norm(v, p=float('inf'))\n",
        "print(f\"Inf norm: {linf}\")\n",
        "print()\n",
        "\n",
        "# Normalize a vector (make length = 1)\n",
        "v_normalized = v / torch.norm(v)\n",
        "print(f\"Normalized: {v_normalized}\")\n",
        "print(f\"Normalized length: {torch.norm(v_normalized)}\\\")\")"
      ],
      "metadata": {
        "id": "7DkPum2AtEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 Other Useful Linear Algebra Operations"
      ],
      "metadata": {
        "id": "859ZOEVvtEcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix properties and operations\n",
        "A = torch.tensor([[1.0, 2.0],\n",
        "                  [3.0, 4.0]])\n",
        "\n",
        "print(f\"Matrix A:\\\\n{A}\")\n",
        "print()\n",
        "\n",
        "# Trace (sum of diagonal)\n",
        "print(f\"Trace: {torch.trace(A)}\")\n",
        "\n",
        "# Determinant\n",
        "print(f\"Determinant: {torch.det(A)}\")\n",
        "\n",
        "# Matrix inverse\n",
        "A_inv = torch.inverse(A)\n",
        "print(f\"Inverse:\\\\n{A_inv}\")\n",
        "\n",
        "# Verify: A @ A^(-1) = I\n",
        "print(f\"A @ A_inv:\\\\n{A @ A_inv}\")"
      ],
      "metadata": {
        "id": "7UrUUk1ztEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix decompositions (useful for advanced ML)\n",
        "\n",
        "# Eigenvalues and eigenvectors\n",
        "eigenvalues, eigenvectors = torch.linalg.eig(A)\n",
        "print(\"=== Eigendecomposition ===\")\n",
        "print(f\"Eigenvalues: {eigenvalues}\")\n",
        "print(f\"Eigenvectors:\\\\n{eigenvectors}\")\n",
        "print()\n",
        "\n",
        "# Singular Value Decomposition (SVD)\n",
        "# Used in: dimensionality reduction, matrix approximation\n",
        "B = torch.randn(3, 4)\n",
        "U, S, Vh = torch.linalg.svd(B)\n",
        "print(\"=== SVD ===\")\n",
        "print(f\"Original shape: {B.shape}\")\n",
        "print(f\"U shape: {U.shape}\")\n",
        "print(f\"S (singular values): {S}\")\n",
        "print(f\"Vh shape: {Vh.shape}\")"
      ],
      "metadata": {
        "id": "1wyWbW6FtEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part 8: Einstein Summation (einsum) - The Swiss Army Knife\n",
        "\n",
        "`einsum` is a powerful, compact notation for expressing tensor operations.\n",
        "\n",
        "### Why Learn einsum?\n",
        "1. **Expressiveness**: One line can replace many operations\n",
        "2. **Readability**: Once you know it, it's clearer than chains of ops\n",
        "3. **Efficiency**: PyTorch optimizes einsum operations\n",
        "4. **Flexibility**: Handles complex tensor contractions easily\n",
        "\n",
        "### The Notation\n",
        "\n",
        "```\n",
        "torch.einsum(\"subscripts\", tensor1, tensor2, ...)\n",
        "\n",
        "Subscripts use letters to label dimensions:\n",
        "- Same letter = those dimensions are aligned/contracted\n",
        "- Letter only on left = kept in output\n",
        "- Letter on both sides = summed over (contracted)\n",
        "```"
      ],
      "metadata": {
        "id": "gazUKlANtEcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1 Basic einsum Operations"
      ],
      "metadata": {
        "id": "8H7SgP34tEcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Sum all elements\n",
        "a = torch.tensor([1, 2, 3, 4])\n",
        "result = torch.einsum('i->', a)  # 'i' goes in, nothing comes out\n",
        "print(f\"Sum all: einsum('i->', {a.tolist()}) = {result}\")\n",
        "print(f\"Same as: a.sum() = {a.sum()}\")\n",
        "print()\n",
        "\n",
        "# 2. Element-wise operations (identity)\n",
        "result = torch.einsum('i->i', a)  # 'i' in, 'i' out\n",
        "print(f\"Identity: einsum('i->i', a) = {result}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "7x2P15iitEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Matrix transpose\n",
        "A = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "result = torch.einsum('ij->ji', A)  # Swap i and j\n",
        "print(f\"Transpose: einsum('ij->ji', A) =\\\\n{result}\")\n",
        "print(f\"Same as: A.T =\\\\n{A.T}\")\n",
        "print()\n",
        "\n",
        "# 4. Sum over specific axis\n",
        "result = torch.einsum('ij->i', A)  # Keep i, sum over j\n",
        "print(f\"Sum rows: einsum('ij->i', A) = {result}\")\n",
        "print(f\"Same as: A.sum(dim=1) = {A.sum(dim=1)}\")\n",
        "print()\n",
        "\n",
        "result = torch.einsum('ij->j', A)  # Keep j, sum over i\n",
        "print(f\"Sum cols: einsum('ij->j', A) = {result}\")\n",
        "print(f\"Same as: A.sum(dim=0) = {A.sum(dim=0)}\")"
      ],
      "metadata": {
        "id": "EDhOVAn_tEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Extract diagonal\n",
        "M = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6],\n",
        "                  [7, 8, 9]])\n",
        "result = torch.einsum('ii->i', M)  # Same index for both dims\n",
        "print(f\"Diagonal: einsum('ii->i', M) = {result}\")\n",
        "print(f\"Same as: torch.diag(M) = {torch.diag(M)}\")\n",
        "print()\n",
        "\n",
        "# 6. Trace (sum of diagonal)\n",
        "result = torch.einsum('ii->', M)  # Same index, no output dims\n",
        "print(f\"Trace: einsum('ii->', M) = {result}\")\n",
        "print(f\"Same as: torch.trace(M) = {torch.trace(M)}\")"
      ],
      "metadata": {
        "id": "Nbq7s3SltEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2 Two-Tensor einsum Operations"
      ],
      "metadata": {
        "id": "kXdGY-GItEcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Element-wise multiplication\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([4, 5, 6])\n",
        "\n",
        "result = torch.einsum('i,i->i', a, b)  # Same index, keep it\n",
        "print(f\"Element-wise: einsum('i,i->i', a, b) = {result}\")\n",
        "print(f\"Same as: a * b = {a * b}\")\n",
        "print()\n",
        "\n",
        "# Dot product (inner product)\n",
        "result = torch.einsum('i,i->', a, b)  # Same index, sum it out\n",
        "print(f\"Dot product: einsum('i,i->', a, b) = {result}\")\n",
        "print(f\"Same as: torch.dot(a, b) = {torch.dot(a, b)}\")\n",
        "print()\n",
        "\n",
        "# Outer product\n",
        "result = torch.einsum('i,j->ij', a, b)  # Different indices, keep both\n",
        "print(f\"Outer product: einsum('i,j->ij', a, b) =\\\\n{result}\")\n",
        "print(f\"Same as: torch.outer(a, b) =\\\\n{torch.outer(a, b)}\")"
      ],
      "metadata": {
        "id": "hNSD1PbNtEcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MATRIX MULTIPLICATION with einsum\n",
        "A = torch.tensor([[1, 2],\n",
        "                  [3, 4]], dtype=torch.float32)\n",
        "B = torch.tensor([[5, 6],\n",
        "                  [7, 8]], dtype=torch.float32)\n",
        "\n",
        "# ik,kj->ij means: multiply along shared 'k' dimension\n",
        "result = torch.einsum('ik,kj->ij', A, B)\n",
        "print(f\"Matrix multiply: einsum('ik,kj->ij', A, B) =\\\\n{result}\")\n",
        "print(f\"Same as: A @ B =\\\\n{A @ B}\")\n",
        "print()\n",
        "\n",
        "# Understanding the notation:\n",
        "# A has shape (i, k) = (2, 2)\n",
        "# B has shape (k, j) = (2, 2)\n",
        "# k is shared (contracted/summed)\n",
        "# Result has shape (i, j) = (2, 2)"
      ],
      "metadata": {
        "id": "SUYOqtaAtEcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3 einsum for Deep Learning\n",
        "\n",
        "Common patterns used in neural networks."
      ],
      "metadata": {
        "id": "rCOIXNDWtEcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batched matrix multiplication\n",
        "batch = 32\n",
        "A = torch.randn(batch, 10, 20)  # (batch, i, k)\n",
        "B = torch.randn(batch, 20, 30)  # (batch, k, j)\n",
        "\n",
        "result = torch.einsum('bik,bkj->bij', A, B)  # b is batch dim\n",
        "print(f\"Batched matmul: einsum('bik,bkj->bij') = {result.shape}\")\n",
        "print(f\"Same as: torch.bmm(A, B) = {torch.bmm(A, B).shape}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "Va7w-5UvtEcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention mechanism (simplified)\n",
        "# Q @ K^T / sqrt(d) then softmax, then @ V\n",
        "\n",
        "batch = 4\n",
        "seq_len = 10\n",
        "d_model = 64\n",
        "\n",
        "Q = torch.randn(batch, seq_len, d_model)  # Queries\n",
        "K = torch.randn(batch, seq_len, d_model)  # Keys\n",
        "V = torch.randn(batch, seq_len, d_model)  # Values\n",
        "\n",
        "# Step 1: Q @ K^T (attention scores)\n",
        "# 'bqd,bkd->bqk' means: batch, query_pos, key_pos\n",
        "scores = torch.einsum('bqd,bkd->bqk', Q, K) / (d_model ** 0.5)\n",
        "print(f\"Attention scores shape: {scores.shape}\")\n",
        "\n",
        "# Step 2: Apply softmax\n",
        "attention = torch.softmax(scores, dim=-1)\n",
        "\n",
        "# Step 3: Attention @ Values\n",
        "# 'bqk,bkd->bqd' means: apply attention weights to values\n",
        "output = torch.einsum('bqk,bkd->bqd', attention, V)\n",
        "print(f\"Attention output shape: {output.shape}\")"
      ],
      "metadata": {
        "id": "839QQpoftEcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4 einsum Cheatsheet\n",
        "\n",
        "| Operation | einsum | Equivalent |\n",
        "|-----------|--------|------------|\n",
        "| Sum all | `'i->'` | `x.sum()` |\n",
        "| Column sum | `'ij->j'` | `x.sum(dim=0)` |\n",
        "| Row sum | `'ij->i'` | `x.sum(dim=1)` |\n",
        "| Transpose | `'ij->ji'` | `x.T` |\n",
        "| Diagonal | `'ii->i'` | `torch.diag(x)` |\n",
        "| Trace | `'ii->'` | `torch.trace(x)` |\n",
        "| Dot product | `'i,i->'` | `torch.dot(a, b)` |\n",
        "| Outer product | `'i,j->ij'` | `torch.outer(a, b)` |\n",
        "| Element-wise | `'i,i->i'` | `a * b` |\n",
        "| Matrix multiply | `'ik,kj->ij'` | `A @ B` |\n",
        "| Batch matmul | `'bik,bkj->bij'` | `torch.bmm(A, B)` |\n",
        "| Matrix-vector | `'ij,j->i'` | `A @ v` |\n",
        "| Attention scores | `'bqd,bkd->bqk'` | `Q @ K.transpose(-1,-2)` |"
      ],
      "metadata": {
        "id": "mWnzTZdLtEcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part 9: Common Deep Learning Operations\n",
        "\n",
        "Operations you'll use constantly in neural networks!"
      ],
      "metadata": {
        "id": "cRqMdwYTtEcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1 Activation Functions\n",
        "\n",
        "Non-linear functions that give neural networks their power!\n",
        "\n",
        "```\n",
        "Why activations matter:\n",
        "- Without them, stacked linear layers = single linear layer\n",
        "- They introduce non-linearity\n",
        "- Different activations have different properties\n",
        "```"
      ],
      "metadata": {
        "id": "kPfR8C6DtEcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "x = torch.linspace(-5, 5, 100)\n",
        "\n",
        "# ReLU: max(0, x) - Most popular for hidden layers\n",
        "relu = F.relu(x)\n",
        "\n",
        "# Sigmoid: 1/(1+e^-x) - Output in (0, 1), for binary classification\n",
        "sigmoid = torch.sigmoid(x)\n",
        "\n",
        "# Tanh: (e^x - e^-x)/(e^x + e^-x) - Output in (-1, 1)\n",
        "tanh = torch.tanh(x)\n",
        "\n",
        "# LeakyReLU: max(0.01x, x) - Prevents \"dying ReLU\"\n",
        "leaky_relu = F.leaky_relu(x, negative_slope=0.1)\n",
        "\n",
        "# GELU: Used in transformers\n",
        "gelu = F.gelu(x)\n",
        "\n",
        "# Plot all activations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 7))\n",
        "activations = [('ReLU', relu), ('Sigmoid', sigmoid), ('Tanh', tanh),\n",
        "               ('LeakyReLU', leaky_relu), ('GELU', gelu)]\n",
        "\n",
        "for ax, (name, activation) in zip(axes.flat, activations):\n",
        "    ax.plot(x.numpy(), activation.numpy(), linewidth=2)\n",
        "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "    ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "    ax.set_title(name, fontsize=12)\n",
        "    ax.set_xlim(-5, 5)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 2].axis('off')  # Hide empty subplot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vWBE6KPptEcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2 Softmax - For Multi-class Classification\n",
        "\n",
        "Converts raw scores (logits) to probabilities that sum to 1."
      ],
      "metadata": {
        "id": "7ZrBcSd2tEcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax: e^xi / sum(e^xj)\n",
        "logits = torch.tensor([2.0, 1.0, 0.1])  # Raw network outputs\n",
        "probabilities = F.softmax(logits, dim=0)\n",
        "\n",
        "print(f\"Logits (raw scores): {logits}\")\n",
        "print(f\"Probabilities: {probabilities}\")\n",
        "print(f\"Sum of probabilities: {probabilities.sum()}\")  # Always 1!\n",
        "print()\n",
        "\n",
        "# For batched data\n",
        "batch_logits = torch.randn(4, 10)  # 4 samples, 10 classes\n",
        "batch_probs = F.softmax(batch_logits, dim=1)  # Softmax along class dim\n",
        "\n",
        "print(f\"Batch logits shape: {batch_logits.shape}\")\n",
        "print(f\"Batch probabilities shape: {batch_probs.shape}\")\n",
        "print(f\"Each row sums to: {batch_probs.sum(dim=1)}\")"
      ],
      "metadata": {
        "id": "QMKlPONYtEcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3 Loss Functions\n",
        "\n",
        "Measure how wrong your predictions are."
      ],
      "metadata": {
        "id": "trkK96S5tEcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE Loss - for regression\n",
        "predictions = torch.tensor([2.5, 0.0, 2.1])\n",
        "targets = torch.tensor([3.0, -0.5, 2.0])\n",
        "\n",
        "mse = F.mse_loss(predictions, targets)\n",
        "print(f\"Predictions: {predictions}\")\n",
        "print(f\"Targets: {targets}\")\n",
        "print(f\"MSE Loss: {mse}\")\n",
        "print()\n",
        "\n",
        "# Cross Entropy Loss - for classification\n",
        "# Takes LOGITS (not probabilities!) and CLASS INDICES\n",
        "logits = torch.tensor([[2.0, 0.5, 0.1],    # Sample 1: class 0 has highest logit\n",
        "                       [0.1, 0.3, 2.5]])    # Sample 2: class 2 has highest logit\n",
        "labels = torch.tensor([0, 2])               # True labels: class 0, class 2\n",
        "\n",
        "ce_loss = F.cross_entropy(logits, labels)\n",
        "print(f\"Logits:\\\\n{logits}\")\n",
        "print(f\"True labels: {labels}\")\n",
        "print(f\"Cross Entropy Loss: {ce_loss}\\\")\")"
      ],
      "metadata": {
        "id": "IA4seGz1tEcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4 Normalization Operations\n",
        "\n",
        "Essential for stable training!"
      ],
      "metadata": {
        "id": "4ZFg8nTOtEcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Normalization (for CNNs)\n",
        "# Normalizes across batch dimension\n",
        "batch = torch.randn(32, 64, 8, 8)  # (batch, channels, H, W)\n",
        "batch_norm = torch.nn.BatchNorm2d(64)\n",
        "normalized = batch_norm(batch)\n",
        "print(f\"BatchNorm: {batch.shape} -> {normalized.shape}\")\n",
        "\n",
        "# Layer Normalization (for Transformers)\n",
        "# Normalizes across feature dimensions\n",
        "sequence = torch.randn(32, 10, 512)  # (batch, seq_len, d_model)\n",
        "layer_norm = torch.nn.LayerNorm(512)\n",
        "normalized = layer_norm(sequence)\n",
        "print(f\"LayerNorm: {sequence.shape} -> {normalized.shape}\")\n",
        "print()\n",
        "\n",
        "# Manual normalization (standardization)\n",
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "mean = x.mean()\n",
        "std = x.std()\n",
        "x_normalized = (x - mean) / std\n",
        "print(f\"Original: {x}\")\n",
        "print(f\"Normalized: {x_normalized}\")\n",
        "print(f\"New mean: {x_normalized.mean():.6f}, std: {x_normalized.std():.6f}\\\")\")"
      ],
      "metadata": {
        "id": "xRXMssXztEcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.5 Clipping and Dropout"
      ],
      "metadata": {
        "id": "P5KyyG8stEcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clipping - prevent extreme values\n",
        "x = torch.tensor([-5.0, -1.0, 0.0, 1.0, 5.0])\n",
        "\n",
        "# Clip to range [-2, 2]\n",
        "clipped = torch.clamp(x, min=-2, max=2)\n",
        "print(f\"Original: {x}\")\n",
        "print(f\"Clipped to [-2, 2]: {clipped}\")\n",
        "print()\n",
        "\n",
        "# Gradient clipping (important for training stability!)\n",
        "gradients = torch.randn(100) * 10  # Large gradients\n",
        "max_norm = 1.0\n",
        "clipped_grads = gradients.clamp(-max_norm, max_norm)\n",
        "print(f\"Gradient norm before: {gradients.norm():.4f}\")\n",
        "print(f\"Gradient norm after clipping: {clipped_grads.norm():.4f}\")\n",
        "print()\n",
        "\n",
        "# Dropout - randomly zero out elements (regularization)\n",
        "x = torch.ones(10)\n",
        "dropout = F.dropout(x, p=0.5, training=True)  # 50% dropout\n",
        "print(f\"Original: {x}\")\n",
        "print(f\"After dropout (p=0.5): {dropout}\")\n",
        "print(\"Note: Remaining values are scaled up to maintain expected sum!\")"
      ],
      "metadata": {
        "id": "8GCqympvtEcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part 10: Practical PyTorch Patterns\n",
        "\n",
        "Real-world code patterns you'll use constantly!"
      ],
      "metadata": {
        "id": "awBzerKvtEcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.1 Weight Initialization\n",
        "\n",
        "Proper initialization prevents vanishing/exploding gradients!"
      ],
      "metadata": {
        "id": "TSYlcvbitEcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Xavier/Glorot initialization - good for tanh, sigmoid\n",
        "# Keeps variance stable through layers\n",
        "W_xavier = torch.empty(256, 512)\n",
        "nn.init.xavier_uniform_(W_xavier)\n",
        "print(f\"Xavier uniform - mean: {W_xavier.mean():.6f}, std: {W_xavier.std():.4f}\")\n",
        "\n",
        "nn.init.xavier_normal_(W_xavier)\n",
        "print(f\"Xavier normal  - mean: {W_xavier.mean():.6f}, std: {W_xavier.std():.4f}\")\n",
        "print()\n",
        "\n",
        "# Kaiming/He initialization - best for ReLU\n",
        "# Accounts for ReLU zeroing half the values\n",
        "W_kaiming = torch.empty(256, 512)\n",
        "nn.init.kaiming_uniform_(W_kaiming, mode='fan_in', nonlinearity='relu')\n",
        "print(f\"Kaiming uniform - mean: {W_kaiming.mean():.6f}, std: {W_kaiming.std():.4f}\")\n",
        "\n",
        "nn.init.kaiming_normal_(W_kaiming, mode='fan_in', nonlinearity='relu')\n",
        "print(f\"Kaiming normal  - mean: {W_kaiming.mean():.6f}, std: {W_kaiming.std():.4f}\")"
      ],
      "metadata": {
        "id": "-nTjZO3-tEcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.2 Autograd Basics - Automatic Differentiation\n",
        "\n",
        "PyTorch tracks operations and computes gradients automatically!"
      ],
      "metadata": {
        "id": "UkreO0vNtEcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# requires_grad=True tells PyTorch to track operations\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "print(f\"x = {x}\")\n",
        "\n",
        "# Perform some operations\n",
        "y = x ** 2          # y = x^2\n",
        "z = y.sum()         # z = sum(y) = x1^2 + x2^2 = 4 + 9 = 13\n",
        "print(f\"y = x^2 = {y}\")\n",
        "print(f\"z = sum(y) = {z}\")\n",
        "\n",
        "# Compute gradients: dz/dx\n",
        "z.backward()\n",
        "\n",
        "# dz/dx = d(x1^2 + x2^2)/dx = [2*x1, 2*x2] = [4, 6]\n",
        "print(f\"Gradient dz/dx = {x.grad}\")\n",
        "print(\"This is [2*2, 2*3] = [4, 6]\\\")\")"
      ],
      "metadata": {
        "id": "9u7y-7eZtEcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable gradient tracking (for inference)\n",
        "model_weights = torch.randn(10, 5, requires_grad=True)\n",
        "\n",
        "# During training, gradients are tracked\n",
        "output_train = model_weights @ torch.randn(5, 3)\n",
        "print(f\"Training - requires_grad: {output_train.requires_grad}\")\n",
        "\n",
        "# During inference, disable for speed + memory\n",
        "with torch.no_grad():\n",
        "    output_eval = model_weights @ torch.randn(5, 3)\n",
        "    print(f\"Inference - requires_grad: {output_eval.requires_grad}\")\n",
        "\n",
        "# Detach: create a tensor that shares data but doesn't track gradients\n",
        "detached = output_train.detach()\n",
        "print(f\"Detached - requires_grad: {detached.requires_grad}\")"
      ],
      "metadata": {
        "id": "ZHRpK6ovtEcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.3 Common Code Snippets\n",
        "\n",
        "Copy-paste ready patterns for everyday use!"
      ],
      "metadata": {
        "id": "L03m1IDVtEcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === PATTERN 1: One-hot encoding ===\n",
        "num_classes = 5\n",
        "labels = torch.tensor([0, 2, 4, 1])\n",
        "one_hot = F.one_hot(labels, num_classes)\n",
        "print(f\"One-hot encoding:\")\n",
        "print(f\"Labels: {labels}\")\n",
        "print(f\"One-hot:\\\\n{one_hot}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "AE35xq2XtEcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === PATTERN 2: Top-k predictions ===\n",
        "logits = torch.randn(4, 10)  # 4 samples, 10 classes\n",
        "probs = F.softmax(logits, dim=1)\n",
        "\n",
        "# Get top 3 predictions for each sample\n",
        "top_probs, top_indices = torch.topk(probs, k=3, dim=1)\n",
        "print(f\"Top 3 predictions:\")\n",
        "print(f\"Probabilities:\\\\n{top_probs}\")\n",
        "print(f\"Class indices:\\\\n{top_indices}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "iyUzjmUVtEcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === PATTERN 3: Masking (e.g., for attention) ===\n",
        "seq_len = 5\n",
        "# Causal mask: can only attend to past positions\n",
        "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "print(f\"Causal attention mask (True = masked):\\\\n{causal_mask}\")\n",
        "\n",
        "# Apply mask to attention scores\n",
        "attn_scores = torch.randn(seq_len, seq_len)\n",
        "masked_scores = attn_scores.masked_fill(causal_mask, float('-inf'))\n",
        "print(f\"\\\\nMasked attention scores:\\\\n{masked_scores}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "uyI7iudLtEcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === PATTERN 4: Padding sequences ===\n",
        "# Variable length sequences need padding for batching\n",
        "seq1 = torch.tensor([1, 2, 3])\n",
        "seq2 = torch.tensor([4, 5])\n",
        "seq3 = torch.tensor([6, 7, 8, 9])\n",
        "\n",
        "# Pad to same length\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "padded = pad_sequence([seq1, seq2, seq3], batch_first=True, padding_value=0)\n",
        "print(f\"Padded sequences:\\\\n{padded}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "CrcsLOgttEcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === PATTERN 5: Gathering specific elements ===\n",
        "# Useful for selecting specific indices (e.g., getting loss for correct class)\n",
        "batch_size = 4\n",
        "num_classes = 5\n",
        "logits = torch.randn(batch_size, num_classes)\n",
        "labels = torch.tensor([0, 2, 1, 4])\n",
        "\n",
        "# Get the logit for each correct class\n",
        "selected = logits.gather(dim=1, index=labels.unsqueeze(1))\n",
        "print(f\"Logits:\\\\n{logits}\")\n",
        "print(f\"Labels: {labels}\")\n",
        "print(f\"Selected logits for correct classes: {selected.squeeze()}\\\")\")"
      ],
      "metadata": {
        "id": "QzhBEFa4tEcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Summary: Your PyTorch Tensor Toolkit\n",
        "\n",
        "## Quick Reference Card\n",
        "\n",
        "### Creating Tensors\n",
        "```python\n",
        "torch.tensor([1, 2, 3])          # From list\n",
        "torch.zeros(3, 4)                # All zeros\n",
        "torch.ones(3, 4)                 # All ones\n",
        "torch.randn(3, 4)                # Normal distribution\n",
        "torch.rand(3, 4)                 # Uniform [0, 1)\n",
        "torch.arange(0, 10, 2)           # Range\n",
        "torch.eye(4)                     # Identity matrix\n",
        "```\n",
        "\n",
        "### Tensor Properties\n",
        "```python\n",
        "t.shape / t.size()               # Dimensions\n",
        "t.dtype                          # Data type\n",
        "t.device                         # CPU or CUDA\n",
        "t.ndim                           # Number of dimensions\n",
        "t.numel()                        # Total elements\n",
        "```\n",
        "\n",
        "### Reshaping\n",
        "```python\n",
        "t.view(3, 4)                     # Reshape (contiguous)\n",
        "t.reshape(3, 4)                  # Reshape (flexible)\n",
        "t.flatten()                      # 1D\n",
        "t.unsqueeze(0)                   # Add dimension\n",
        "t.squeeze()                      # Remove size-1 dims\n",
        "t.permute(2, 0, 1)               # Reorder dims\n",
        "t.T                              # Transpose\n",
        "```\n",
        "\n",
        "### Operations\n",
        "```python\n",
        "a + b, a - b, a * b, a / b       # Element-wise\n",
        "a @ b                            # Matrix multiply\n",
        "torch.cat([a, b], dim=0)         # Concatenate\n",
        "torch.stack([a, b], dim=0)       # Stack (new dim)\n",
        "t.sum(dim=0)                     # Sum along axis\n",
        "t.mean(), t.std()                # Statistics\n",
        "torch.argmax(t, dim=1)           # Index of max\n",
        "```\n",
        "\n",
        "### Deep Learning Essentials\n",
        "```python\n",
        "F.relu(x)                        # Activation\n",
        "F.softmax(x, dim=1)              # Probabilities\n",
        "F.cross_entropy(logits, labels)  # Loss\n",
        "torch.no_grad()                  # Inference mode\n",
        "x.backward()                     # Compute gradients\n",
        "```\n",
        "\n",
        "### einsum Magic\n",
        "```python\n",
        "torch.einsum('ij->ji', A)        # Transpose\n",
        "torch.einsum('ik,kj->ij', A, B)  # Matrix multiply\n",
        "torch.einsum('bik,bkj->bij', A, B)  # Batch matmul\n",
        "```"
      ],
      "metadata": {
        "id": "LvpFOCiWtEcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Congratulations!\n",
        "\n",
        "You now have a solid foundation in PyTorch tensors and operations. These are the building blocks for everything in deep learning!\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "1. **Build Neural Networks**: Use `torch.nn.Module` to create models\n",
        "2. **Train Models**: Learn the training loop (forward, loss, backward, step)\n",
        "3. **Use DataLoaders**: Efficient data loading with `torch.utils.data`\n",
        "4. **GPU Training**: Scale up with CUDA\n",
        "5. **Advanced Architectures**: CNNs, RNNs, Transformers\n",
        "\n",
        "### Practice Exercises\n",
        "\n",
        "1. Create a function that normalizes a batch of images (subtract mean, divide by std)\n",
        "2. Implement a simple dot-product attention using only tensor operations\n",
        "3. Write a function that creates positional encodings using sin/cos\n",
        "4. Build a mini neural network layer using raw tensor operations (no nn.Linear)\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Deep Learning!**"
      ],
      "metadata": {
        "id": "9HaYP8OatEcY"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}